# 


## Create Traffic Generator and Update color-api

### color-api

#### Adding Hostname Information to color-api
```
docker build -t selcouthonism/color-api-image:1.1.0 .
```
In this step, the color-api application is updated to include hostname information in its responses, which is critical for observing load-balancing behavior later in Docker and Kubernetes. The image is rebuilt with version 1.1.0 to create an immutable, versioned artifact that reflects the code change. Tagging the image explicitly ensures reproducibility and avoids ambiguity when deploying across environments.

#### Running the Updated color-api Locally
```
docker run --rm -d -p 8080:80 selcouthonism/color-api-image:1.1.0
```
The container is started with port mapping 8080:80, exposing the application’s internal port 80 to the host on port 8080. This allows easy local testing while keeping the container configuration production-like. Running in detached mode (-d) ensures the container stays up while tests are performed.

#### Verifying color-api Endpoints
```
curl localhost:8080
curl localhost:8080/api
curl "localhost:8080/api?format=json"
curl "localhost:8080/api?format=text"
```
These curl commands validate that the application responds correctly for different endpoints and formats. The root path confirms the service is running, while /api checks the backend logic. Query parameters are used to test content negotiation behavior, ensuring both JSON and text responses work as expected before moving to container-to-container or Kubernetes networking.

#### Publishing the color-api Image
```
docker push selcouthonism/color-api-image:1.1.0
```
Pushing the image to a registry makes it accessible for other environments, including Docker networks and Kubernetes clusters. This step enforces the best practice of using a centralized image registry rather than relying on local builds. It also ensures that the same image version is used consistently across all tests.

### Traffic Generator

#### Preparing the Traffic Generator Script
```
chmod +x traffic-gen.sh
```
The ```chmod +x``` command makes the shell script executable on the host system, allowing it to be run directly. This step validates that the script itself works independently of Docker, which helps isolate application logic issues from containerization issues. Testing locally first is a key DevOps debugging habit.

#### Running the Traffic Generator Locally
```
./traffic-gen.sh "localhost:8080/api" 0.5
```
Running the script directly against localhost:8080/api verifies that it can successfully generate traffic and display responses. This confirms that the script logic, timing loop, and curl command behave as expected before packaging it into a container. Any failures here should be fixed before moving on to Docker.

#### Building the Traffic Generator Image
```
docker build -t selcouthonism/traffic-generator:1.0.0 .
```
The traffic generator is containerized into its own image, making it portable and environment-agnostic. This allows the same tool to be used in Docker, Kubernetes, and CI pipelines without modification. Versioning the image ensures controlled updates and rollback capability.

#### Validating the Traffic Generator Image
```
docker run --rm selcouthonism/traffic-generator:1.0.0
```
Running the image without arguments confirms that the ENTRYPOINT is correctly configured and that the usage message is displayed. This is a quick sanity check to ensure Docker is invoking the shell script rather than an unexpected runtime. Catching ENTRYPOINT issues early prevents confusing runtime errors later.

#### Publishing the Traffic Generator Image
```
docker push selcouthonism/traffic-generator:1.0.0
```
Pushing the traffic generator image to a registry enables reuse across Docker networks and Kubernetes clusters. This mirrors real-world workflows where test and utility containers are shared across teams. It also ensures that Kubernetes can pull the image without relying on local Docker caches.

### Test Connection with Docker

#### Creating a Docker Network
```
docker network create demo-net
```
A user-defined Docker bridge network is created to enable container-to-container communication via DNS. This allows containers to refer to each other by name rather than IP address, closely mimicking Kubernetes service discovery. Using a custom network is a best practice over the default bridge.

#### Running color-api in the Docker Network
```
docker run --rm -d -p 8080:80 --name color-api --network demo-net selcouthonism/color-api-image:1.1.0
```
The color-api container is started inside the demo-net network with a fixed container name. This makes the service discoverable via the hostname color-api from other containers on the same network. Port mapping is kept only for optional host access, not for inter-container communication.

#### Running Traffic Generator Against color-api
```
docker run --rm --network demo-net selcouthonism/traffic-generator:1.0.0 "http://color-api/api" 0.5
```
The traffic generator container is started on the same Docker network and targets the color-api service by name. This demonstrates Docker’s built-in DNS resolution and validates service-to-service communication without relying on localhost. Continuous requests confirm stable connectivity and response handling.

### Test Connection with Kubernetes

#### Deploying color-api to Kubernetes
```
kubectl apply -f color-api-deploy.yaml 
```
The Deployment manifest creates multiple Pods running the color-api image, enabling horizontal scaling and self-healing. Kubernetes manages Pod lifecycle, restarts, and placement across nodes. This step transitions the application from container-only to an orchestrated environment.

#### Exposing color-api via ClusterIP Service
```
kubectl apply -f color-api-svc-clusterip.yaml 
```
The ClusterIP Service provides a stable virtual IP and DNS name for accessing the color-api Pods internally. It abstracts away individual Pod IPs and enables built-in load balancing. This is the standard and recommended way to expose services inside a Kubernetes cluster.

#### Deploying Traffic Generator Pod
```
kubectl apply -f traffic-gen-pod.yaml
```
The traffic generator Pod uses the Service’s ClusterIP to send traffic to color-api, simulating internal client behavior. This validates that service discovery and networking are working correctly inside the cluster. Using a Pod instead of a Deployment keeps the test simple and controlled.

#### Observing Load Balancing
```
kubectl logs -f traffic-gen-pod
```
By streaming the traffic generator logs, you can observe responses coming from different Pod hostnames. This confirms that Kubernetes is distributing traffic across multiple Pods behind the Service. It is a simple but effective way to visualize load balancing in action.

#### Inspecting Service Endpoints
```
kubectl describe svc color-api-svc-clusterip
```
Describing the Service shows the list of Pod IPs currently backing it. When Pods are recreated, these IPs change, but the Service remains stable. This highlights why direct Pod IP access is fragile and why Services are essential in Kubernetes.

#### Using Service Name Instead of Cluster IP
```
spec:
  containers:
  - name: traffic-gen-container
    image: selcouthonism/traffic-generator:1.0.0
    args:
    # Find Cluster IP with 'kubectl get svc' command
    #- '10.104.90.109/api'
    - 'color-api-svc-clusterip/api'
    - '0.5'
```
Updating the traffic generator to use the Service name rather than a Cluster IP ensures resilience and portability. Service names remain constant even when Services or Pods are recreated, while Cluster IPs may change. This is a fundamental Kubernetes best practice.

#### Verifying CoreDNS
```
kubectl get pods -n kube-system
```

```
kubectl logs -n kube-system coredns-66bc5c9577-pkjrn 
```
Inspecting CoreDNS Pods and logs confirms that DNS resolution is functioning inside the cluster. CoreDNS is responsible for resolving Service names to Cluster IPs, enabling service-to-service communication. This step helps diagnose DNS-related connectivity issues.

#### Exposing color-api via NodePort Service
```
kubectl apply -f color-api-svc-nodeport.yaml 
```
Applying the NodePort Service manifest creates a Kubernetes Service that exposes the color-api Pods on a static port (in the NodePort range, typically 30000–32767) on every node in the cluster. This allows traffic coming to any node’s IP on the NodePort to be forwarded to the backend Pods. NodePort is commonly used for local testing, demos, or bare-metal clusters where cloud load balancers are not available.

- **selector:** This ensures the service is correctly routing traffic to the intended pods. If the labels don't match, the service won't find any pods.
- **port and targetPort:** These define how the service listens internally and which port on the pod it forwards traffic to. Misconfigurations here can prevent internal routing.
- **nodePort:** This is the specific port on the node's IP address that external traffic will hit. If this is incorrect, or if there's a firewall blocking this port, external access will fail.

#### Listing Cluster Nodes and Their IPs
```
kubectl get nodes -o wide
```
This command shows detailed node information, including internal IP addresses. These IPs represent the network interface on which the NodePort is exposed. In single-node setups like Minikube, this IP belongs to the VM or container running Kubernetes rather than the host machine itself.

```
NAME       STATUS   ROLES           AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
minikube   Ready    control-plane   3d21h   v1.34.0   192.168.49.2   <none>        Ubuntu 22.04.5 LTS   6.10.14-linuxkit   docker://28.4.0
```

#### Accessing the NodePort via Internal IP (Linux)
```
curl 192.168.49.2:30007
```
On Linux systems, the node’s INTERNAL-IP is directly reachable from the host, allowing you to access the application using ```curl <INTERNAL-IP>:<NodePort>```. This works because Minikube runs with networking that exposes the node IP to the host. On macOS and Windows, Minikube runs inside a VM with NAT, so this direct access path does not work.

#### Inspecting Pod Placement and Networking
```
kubectl get pods -o wide 
kubectl get pods -n kube-system -o wide 
```
Running ```kubectl get pods -o wide``` shows which Pods are backing the Service and their assigned Pod IPs, while querying the kube-system namespace reveals where core components like CoreDNS and kube-proxy are running. This helps verify that Pods are healthy and distributed correctly, and that cluster networking components required for NodePort routing are operational.

#### Using Minikube Service on macOS and Windows
```
minikube service color-api-svc-nodeport --url
```
Because macOS and Windows do not allow direct access to the Minikube node IP, the minikube service command creates a local tunnel that forwards traffic from the host to the NodePort Service. This abstracts away VM networking complexity and provides a host-accessible URL that behaves as if the Service were exposed directly.

#### Accessing color-api Through Minikube Tunnel
```
curl http://127.0.0.1:50649/api
```
The URL returned by ```minikube service color-api-svc-nodeport --url``` maps a local host port to the NodePort running inside Minikube. Sending requests to this URL successfully reaches the color-api Service and is functionally equivalent to calling ```<node-ip>:<nodeport>``` on Linux. This approach is the recommended and most reliable way to test NodePort Services on macOS and Windows.

#### Creating the ExternalName Service for Google
```
kubectl apply -f google-externalname.yaml
```
Applying the google-externalname.yaml manifest creates a Kubernetes Service of type ExternalName that maps a service name inside the cluster to an external DNS name (for example, www.google.com). Instead of routing traffic to Pods or endpoints, Kubernetes configures CoreDNS to return a CNAME record when this service name is queried. This allows in-cluster applications to access external services using standard Kubernetes service naming conventions.

An ExternalName Service is needed when an application running inside Kubernetes must access an external system (such as a SaaS API, managed database, or legacy service) while still using Kubernetes-native service discovery and configuration patterns. It allows teams to decouple application code from environment-specific endpoints by referencing a stable service name instead of hard-coding external DNS names. This is especially useful in GitOps and multi-environment setups where external dependencies differ between dev, staging, and production. Since ExternalName works purely via DNS and does not proxy traffic, it is best suited for simple integrations where load balancing, health checks, and traffic control are handled outside the cluster.

#### Deploying the Traffic Generator Pod
```
kubectl apply -f traffic-gen-pod.yaml
```
Applying the traffic generator Pod manifest starts a Pod inside the cluster that will act as a client for testing the ExternalName Service. This Pod simulates an internal workload attempting to reach an external dependency. Using a Pod ensures the test reflects real in-cluster networking and DNS behavior rather than host-level resolution.

#### Accessing the Traffic Generator Pod Shell
```
kubectl exec -it traffic-gen-pod -- sh
```
Executing an interactive shell inside the traffic generator Pod allows direct inspection and manual testing from within the Kubernetes network. This step is useful for debugging DNS resolution, connectivity, and HTTP behavior without modifying application code. It provides a controlled environment to verify how Kubernetes resolves the ExternalName Service.

#### Calling the ExternalName Service
```
curl google-svc-externalname
curl -v https://google-svc-externalname
```
Running ```curl google-svc-externalname``` triggers a DNS lookup handled by CoreDNS, which resolves the service name to the external DNS target defined in the ExternalName Service. The request is sent directly from the Pod to Google, with no proxying or load balancing performed by Kubernetes. This demonstrates how ExternalName Services act purely as DNS aliases, making them ideal for abstracting external dependencies behind stable, in-cluster service names.

### Clean Resources

#### Removing Kubernetes Resources
```
kubectl delete -f color-api-deploy.yaml
kubectl delete -f color-api-svc-clusterip.yaml 
kubectl delete -f color-api-svc-nodeport.yaml
kubectl delete -f google-externalname.yaml
kubectl delete -f traffic-gen-pod.yaml
```
Deleting the Deployment, Service, and traffic generator Pod cleans up all resources created during testing. This prevents resource leaks and ensures the cluster returns to a known clean state. Regular cleanup is essential in shared or resource-constrained Kubernetes environments.