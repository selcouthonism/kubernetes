# Kubernetes

Kubernetes (often abbreviated as K8s) is an open-source container orchestration platform that automates the deployment, scaling, management, and operation of application containers. Originally developed by Google and now maintained by the Cloud Native Computing Foundation (CNCF), Kubernetes has become the industry standard for managing containerized applications.

Kubernetes helps you:
- Deploy applications consistently across environments.
- Scale up or down based on traffic or load. (Autoscaling)
- Self-heal by automatically restarting failed containers. (Health check)
- Distribute traffic and load-balance requests.
- Roll out updates without downtime using rolling deployments.
- Isolate workloads using namespaces and RBAC (Role-Based Access Control).
- Manage resources efficiently using scheduling and quotas.

Why Use Kubernetes?
- **Portability**: Works across public clouds, on-prem, and hybrid.
- **Scalability**: Easily handles large applications and traffic.
- **Reliability**: Automatically recovers from failures.
- **DevOps-friendly**: Supports CI/CD, GitOps, and infrastructure as code.
- **Ecosystem**: Integrates with Helm, Prometheus, Istio, ArgoCD, and more.

Kubernetes handles scalability and resilience. Kubernetes uses load balancing to distribute traffic across different pods, and that Replica Sets are crucial for ensuring a desired number of pods are always running, thus maintaining high availability.

When Not to Use Kubernetes?
- For very small projects or solo apps — it's overkill.
- If your team lacks operational experience — it has a learning curve.
- If platform-as-a-service (like Heroku, Fly.io, or App Engine) already meets your needs.

## The Kubernetes Architecture
The Kubernetes system, or Kubernetes architecture, revolves around a master-worker (control plane–node) model where different components collaborate to run and manage containerized applications.

Let’s break down the Kubernetes system (k8s) into its key components grouped by function:

### Control Plane (System “Brain”)
In Kubernetes, the control plane is the brain of the cluster. It is responsible for making global decisions (like scheduling workloads), detecting and responding to cluster state changes, and ensuring that the actual state of the cluster continuously converges to the desired state defined by users. Control plane components do not run application workloads; instead, they coordinate and manage worker nodes.

Its primary role is to maintain the desired state of the cluster. This means it continuously works to ensure that the actual state of your cluster matches the configuration you've defined. For example, if you say you want three replicas of an application running, the Control Plane makes sure that happens and keeps it that way. 

**Control Plane Components:**

The control plane enforces Kubernetes’ declarative model by continuously reconciling desired and actual state. Its components are designed around separation of concerns, high availability, and strong consistency—making Kubernetes a reliable platform for running distributed systems in production.

#### 1. kube-apiserver
The **kube-apiserver** is the front door of the Kubernetes control plane and the only component that directly exposes the Kubernetes API. All requests - whether from kubectl, CI/CD pipelines, controllers, or nodes — go through the API server. It handles authentication, authorization (RBAC), admission control, and validation, and it persists accepted state changes to **etcd**. In production, it is typically deployed in a highly available configuration.

(The API server is the main entry point for all operations in Kubernetes, exposing the Kubernetes API for creating, updating, and managing resources. It's like the central hub for all communication within the cluster.)

- The **front door** to the Kubernetes control plane.
- All components communicate through this HTTP-based API.
- Validates and processes REST requests.
- Stateless — can be horizontally scaled.

#### 2. etcd
**etcd** is a distributed, strongly consistent key-value store that acts as Kubernetes’ source of truth. It stores all cluster state, including configuration data, secrets, metadata, and desired/actual state of resources. Because **etcd** consistency directly affects cluster correctness, it requires careful management: regular backups, encryption at rest, and low-latency, reliable networking.

- The cluster’s **key-value store** (distributed and consistent).
- Stores all cluster state and configuration (e.g., pods, deployments, secrets).
- Highly available and backed up regularly.

#### 3. kube-scheduler
The **kube-scheduler** is responsible for assigning newly created Pods to worker nodes. It evaluates available nodes based on resource requirements, constraints (CPU, memory, GPU), affinities, taints and tolerations, and custom scheduling policies. Once a suitable node is selected, the scheduler records the decision in the API server, after which the Pod is started by the node’s kubelet.

- Assigns pods to nodes.
- Considers resources (CPU, memory), constraints, affinity/anti-affinity, taints/tolerations.

#### 4. kube-controller-manager
The **kube-controller-manager** runs multiple controllers that continuously monitor cluster state and reconcile it toward the desired state. Examples include the ReplicaSet controller (ensures the correct number of Pod replicas), Node controller (detects node failures), and Endpoint controller (manages service endpoints). Each controller follows a control loop pattern: observe → compare → act, making Kubernetes highly resilient and automated.

Example controllers:
- NodeController: Manages node availability.
- ReplicationController: Ensures correct number of pod replicas.
- EndpointController: Manages Service endpoints.
- ServiceAccountController: Creates default accounts and tokens.

#### 5. cloud-controller-manager (optional)
The cloud-controller-manager integrates Kubernetes with cloud provider APIs (AWS, GCP, Azure, etc.). It manages cloud-specific resources such as load balancers, persistent volumes, and node lifecycle events without embedding cloud logic directly into core Kubernetes components. This separation allows Kubernetes to remain cloud-agnostic while still supporting deep cloud-native integrations.

- Abstracts cloud provider-specific logic (e.g., load balancer provisioning on AWS, GCP).
- Only needed for cloud-hosted clusters.

### Node Components (Worker Nodes)
In Kubernetes, worker nodes (also called node components) are the physical or virtual machines where application workloads actually run. Each worker node hosts the services required to run Pods, communicate with the control plane, and manage networking and containers. Unlike control plane components, node components focus on executing and maintaining application state rather than deciding what that state should be.

#### 1. kubelet
The **Kubelet** acts as an agent that communicates with the Control Plane, ensuring containers are running as defined. It is the primary agent running on every worker node. It watches the Kubernetes API server for Pod specifications assigned to its node and ensures that the containers described in those Pods are running and healthy. The kubelet interacts with the container runtime to start, stop, and monitor containers, performs health checks (liveness and readiness probes), mounts volumes, and reports node and Pod status back to the control plane. It does not make scheduling decisions. It only executes what has already been scheduled.

- Node agent — ensures containers in pods are running.
- Communicates with the API server.
- Reports node and pod status.

#### 2. kube-proxy
**kube-proxy** is responsible for implementing Kubernetes Service networking on each node. It manages network rules (typically using iptables, IPVS, or eBPF) to route traffic to the correct backend Pods for a Service. This enables stable virtual IPs and built-in load balancing across multiple Pod replicas, even as Pods are created or destroyed. kube-proxy operates at the network layer and ensures that Service discovery works transparently for applications.

- Manages networking and traffic rules on the node (Handles network rules for services).
- Implements services and handles routing (e.g., ClusterIP, NodePort, LoadBalancer).
- Supports IPTables, IPVS, or eBPF.

#### 3. Container Runtime
The **container runtime** is the software responsible for running containers on the node. Kubernetes communicates with the runtime through the Container Runtime Interface (CRI), allowing different implementations to be used interchangeably. Common runtimes include containerd, CRI-O, and (historically) Docker. The runtime pulls container images, creates and isolates containers using Linux primitives (namespaces, cgroups), and manages their lifecycle. From Kubernetes’ perspective, the runtime is an execution engine—completely replaceable as long as it implements CRI.

- The engine that runs containers (e.g., containerd, CRI-O, Docker).
- Kubernetes talks to the runtime through the Container Runtime Interface (CRI).

### Network Model
**Services** in Kubernetes provide a stable network endpoint for a set of Pods. Even if the underlying Pods are created, destroyed, or change their IP addresses, the Service's IP address and DNS name remain constant. This allows other applications or external users to consistently access your application without needing to know the individual Pod IPs. Services act as an abstraction layer, providing a consistent way to access your application regardless of the dynamic nature of Pods.

When applications go up and down, it's crucial to track what's happening and check logs to understand crashes. Services help with this by providing a consistent entry point. Even if a specific pod crashes and a new one replaces it, the Service ensures that traffic continues to flow to the healthy pods without interruption. This means that from the perspective of a user or another application trying to connect, the service remains available, even if individual pods are failing and being replaced.

So, while you still need to monitor individual pods and their logs for debugging, the Service itself provides a layer of resilience and stability, making the application appear continuously available. This is a huge benefit for high-traffic applications, as it minimizes downtime and ensures a smooth user experience.

To summarize, Services act as an abstraction layer, providing a consistent way to access your application regardless of the dynamic nature of Pods. D
- Every pod gets its own IP.
- All pods can talk to each other by default.
- Services provide stable endpoints and optional load balancing.
- Ingress controllers manage incoming HTTP(S) traffic.

## Kubernetes Standard Interfaces 
Kubernetes defines standard interfaces (a.k.a. abstractions or APIs) that allow extensibility, pluggability, and interoperability between Kubernetes and third-party tools. These standard interfaces make Kubernetes a platform for building platforms — meaning you can plug in your own networking, storage, security, scheduling logic, etc., without modifying Kubernetes itself.

### 1. Container Runtime Interface (CRI)
Interface between Kubelet and the container runtime.

Allows Kubernetes to use different container runtimes (e.g., containerd, CRI-O, Docker via cri-dockerd).
- Supports container lifecycle management (create, start, stop).
- gRPC-based interface.
- Examples of CRI-compliant runtimes: containerd, CRI-O, Mirantis cri-dockerd.

### 2. Container Storage Interface (CSI)
Interface between Kubernetes and storage systems.

Enables third-party storage vendors to develop **volume plugins** that work with Kubernetes without modifying the core.
- Supports dynamic provisioning, volume attach/detach, snapshotting, resizing, etc.
- Standardized across multiple orchestrators (Kubernetes, Nomad, etc.).
- Replaces in-tree volume plugins (deprecated since K8s v1.21+).

### 3. Container Network Interface (CNI)
Interface between Kubernetes and network plugins.

Provides network connectivity for pods and supports custom networking behavior (e.g., isolation, routing).
- Every pod gets an IP.
- CNI plugin is executed by Kubelet when a pod is created.
- Popular CNI implementations: Calico, Flannel, Cilium, Weave, Canal.

### 4. Cloud Provider Interface (CPI)
Interface between Kubernetes and cloud infrastructure providers.

- Was originally in-tree (built into Kubernetes).
- Now externalized to out-of-tree cloud controllers (e.g., cloud-controller-manager).
- Examples: aws-cloud-controller-manager, gce-cloud-controller-manager.

Allows Kubernetes to manage cloud-specific resources like:
- Load balancers
- Storage volumes (via CSI)
- Instances/nodes
- Routes and networks
