# Resource Quotas, Requests, and Limits
In Kubernetes, Resource Quotas, Requests, and Limits work together to control resource usage, ensure fair sharing, and maintain cluster stability.

## Requests, and Limits
Resource Requests and Limits are defined at the Pod or container level and describe how much CPU and memory a container needs. A **request** is the amount of resource Kubernetes guarantees for a container and uses for scheduling decisions, meaning the Pod will only be placed on a node that can satisfy its requests. A **limit** is the maximum amount of resource a container is allowed to consume; exceeding a CPU limit results in throttling, while exceeding a memory limit causes the container to be terminated (OOMKilled). **Requests** ensure predictable scheduling, and **limits** prevent individual containers from monopolizing node resources.

## Resource Quotas
Resource Quotas are applied at the **namespace level** and restrict the total amount of resources that all Pods in a namespace can consume. They can limit CPU, memory, storage, and even the number of objects like Pods or Services. Resource Quotas are especially important in multi-tenant clusters to prevent one team or environment from exhausting shared resources, and they work effectively only when Pods define proper requests and limits, enforcing disciplined and predictable resource usage across the cluster.

## Examples

#### Creating ResourceQuotas for dev and prod Namespaces
```
kubectl apply -f dev-ns-limit.yaml -f prod-ns-limit.yaml
```
Applying the namespace manifests creates the dev and prod namespaces along with their associated ResourceQuota objects. These quotas define hard limits on total CPU and memory consumption per namespace, ensuring that each environment has bounded resource usage. This is a foundational control mechanism for multi-environment or multi-team clusters.

#### Listing ResourceQuotas Across the Cluster
```
kubectl get resourcequota -A
```
This command displays all ResourceQuota objects in every namespace. This provides a cluster-wide view of enforced resource limits and helps operators quickly verify which namespaces are governed by quotas. It is especially useful for audits and capacity planning.

#### Inspecting ResourceQuota Details
```
kubectl describe resourcequota dev-resource-quota -n dev
kubectl describe resourcequota prod-resource-quota -n prod
```
Describing the ResourceQuotas for the dev and prod namespaces shows configured limits alongside current usage. This output reveals how much CPU and memory are requested and limited by existing workloads compared to the allowed maximum. It clearly demonstrates how quotas track real-time consumption.

#### Deploying color-api with Resource Limits in dev
```
kubectl apply -f color-api-ns-dev-limit.yaml
```
Applying the color-api-ns-dev-limit.yaml manifest creates a Pod with explicit CPU and memory requests and limits. These resources are defined at the **container level** and are used both by the scheduler and the ResourceQuota controller. This ensures the **Pod can be admitted only if sufficient quota is available**.

#### Verifying Container-Level Resource Settings
```
kubectl describe pod color-api -n dev
```
Describing the Pod confirms that resource requests and limits are correctly applied to the container. This step verifies that resource constraints are not just theoretical but enforced in the running workload. It also demonstrates that quotas operate based on container-level definitions, not Pod-level abstractions.

#### Rejecting a Pod That Exceeds the Quota
```
kubectl apply -f color-api-ns-dev-heavy-limit.yaml
```
Applying the heavy Pod manifest fails because the requested CPU and memory exceed the remaining quota in the dev namespace. Kubernetes blocks Pod creation before scheduling, protecting the namespace from overconsumption. This shows how quotas act as an admission control mechanism.

The **request** is used by the scheduler to decide which node a pod can run on, ensuring that the node has enough available resources to meet the pod's minimum requirements. If requests are too high, the scheduler might not find a suitable node, leading to the pod not being scheduled at all. If limits are not set or are too low, pods can crash abruptly (especially with memory) or be throttled (with CPU), impacting application performance and reliability.

#### Understanding the Quota Error Message
```
Error from server (Forbidden): error when creating "color-api-ns-dev-heavy-limit.yaml": pods "heavy-api" is forbidden: exceeded quota: dev-resource-quota, 
requested: limits.cpu=2,limits.memory=2Gi,requests.cpu=1,requests.memory=1Gi, 
used: limits.cpu=500m,limits.memory=512Mi,requests.cpu=200m,requests.memory=256Mi, 
limited: limits.cpu=2,limits.memory=2Gi,requests.cpu=1,requests.memory=1Gi
```
The error message clearly breaks down requested, used, and limited resources, making it easy to understand why the request was rejected. It shows that even though the per-Pod limits might be valid, the namespace-level quota has already been exhausted. This transparency is critical for debugging and capacity planning.

#### Clearing Existing Pods to Free Quota
```
kubectl delete -f color-api-ns-dev-limit.yaml
```
Deleting the existing Pod removes its resource usage from the namespace quota calculation. This immediately frees up CPU and memory, allowing new workloads to be admitted. ResourceQuotas are dynamically updated based on active resource usage.

#### Deploying a Deployment with Resource Limits
```
kubectl apply -f color-api-depl-ns-dev-limit.yaml
```
Applying the Deployment manifest creates multiple replicas of the color-api Pod, each consuming quota-defined resources. Unlike single Pods, Deployments introduce scaling and rolling update behavior, which directly affects quota consumption. This step sets the stage for understanding quota interaction during rollouts.

#### Verifying Quota Exhaustion
```
kubectl describe resourcequota dev-resource-quota -n dev
```
Describing the ResourceQuota after deployment shows that all available resources are now fully consumed. This confirms that the Deployment replicas collectively use the entire quota. It demonstrates how scaling decisions must account for namespace-level limits.

The USED column shows how much of a particular resource (like CPU or memory) is currently being consumed by pods in the namespace, based on their requests. The HARD column shows the maximum amount of that resource that the namespace is allowed to consume, as defined by the quota.
#### Watching Pod Creation in Real Time
```
kubectl get pods -n dev --watch
```
Using ```kubectl get pods -n dev --watch``` allows observation of Pods being created, scheduled, or failing in real time. This is particularly useful during rollouts and scaling events. It provides immediate feedback on how Kubernetes reacts under quota pressure.

If a pod exceeds its memory limit, it will be terminated and potentially restarted. However, the behavior is a bit different for CPU limits. If a pod tries to use more CPU than its limit, it will be throttled rather than restarted.

#### Triggering a Rolling Update by Changing Image Version
```
kubectl apply -f color-api-depl-ns-dev-limit.yaml
```
Reapplying the Deployment with a new image tag triggers a rolling update, where Kubernetes attempts to create new Pods before terminating old ones. This behavior temporarily increases resource demand. In a quota-constrained namespace, this can cause rollout delays or failures.

#### Monitoring the Deployment Rollout
```
kubectl rollout status deployment color-api-deployment -n dev
```

Output:
```
Waiting for deployment "color-api-deployment" rollout to finish: 2 out of 4 new replicas have been updated...
```
The rollout status command shows that only some replicas are updated, indicating the rollout is blocked by resource constraints. Kubernetes waits for sufficient quota to become available before continuing. This highlights the interaction between rolling updates and namespace quotas.

#### Inspecting the Deployment State
```
kubectl describe deployment color-api-deployment -n dev
```
Describing the Deployment reveals conditions and events explaining why progress is stalled. It shows that the desired state cannot be achieved due to quota enforcement. This helps operators understand that the issue is resource-related, not application-related.

#### Examining ReplicaSets During Rollout
```
kubectl get rs -n dev
```

Output:
```
NAME                              DESIRED   CURRENT   READY   AGE
color-api-deployment-64748898b8   1         1         1       12m
color-api-deployment-848ccb4c5c   4         2         2       5m16s
```
Listing ReplicaSets shows both the old and new ReplicaSets coexisting during the rolling update. The desired replica counts reflect Kubernetesâ€™ attempt to maintain availability while updating. This dual presence increases temporary resource demand.

#### Investigating ReplicaSet Errors
```
kubectl describe rs color-api-deployment-848ccb4c5c -n dev
```

Output:
```
Error creating: pods "color-api-deployment-848ccb4c5c-gfcq4" is forbidden: exceeded quota: dev-resource-quota, requested: limits.cpu=500m,limits.memory=512Mi,requests.memory=256Mi, used: limits.cpu=2,limits.memory=2Gi,requests.memory=1Gi, limited: limits.cpu=2,limits.memory=2Gi,requests.memory=1Gi
```
Describing the new ReplicaSet surfaces the quota error because ReplicaSets are responsible for creating Pods. This confirms that Pod admission is blocked at the ReplicaSet level due to exhausted quota. It reinforces that quota enforcement happens before scheduling.

#### Cleaning Up Resources
```
kubectl delete -f color-api-depl-ns-dev-limit.yaml -f dev-ns-limit.yaml -f prod-ns-limit.yaml 
```
Deleting the Deployment and namespaces removes all associated Pods, ReplicaSets, and ResourceQuotas. This frees cluster resources and returns the environment to a clean state. Proper cleanup is essential to avoid lingering quota usage and unintended resource consumption.

### Steps to prevent exceeded quota issues
#### 1. Implement Monitoring and Alerting: 
Set up monitoring for resource quota usage at the namespace level. This way, the team would receive alerts before quotas are fully consumed, allowing them to take action proactively.

#### 2. Regularly Review Resource Requests and Limits: 
Periodically review the requests and limits set for applications. As applications evolve, their resource needs might change, and these values should be adjusted accordingly.

#### 3. Educate Developers: 
Ensure all developers understand the importance of setting appropriate requests and limits and how ResourceQuotas work. This fosters a culture of resource awareness.

#### 4. Use Limit Ranges: 
Implement **LimitRanges** in namespaces. While ResourceQuotas limit the total resources, LimitRanges can enforce default requests and limits for pods if they aren't explicitly set, and can also set minimum and maximum values for individual pods.
