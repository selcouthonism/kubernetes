# Startup, Liveness, and Readiness Probes
In Kubernetes, Startup, Liveness, and Readiness probes are health-check mechanisms that tell the platform when to start sending traffic, when to restart a container, and how to handle slow-starting applications. Each probe serves a distinct purpose in the container lifecycle.

## Probe Types
### Startup Probe
The startup probe is designed for applications that take a long time to initialize. While this probe is running, Kubernetes disables liveness and readiness checks, preventing premature restarts or traffic routing before the application is fully started. Once the startup probe succeeds, Kubernetes considers the container “started” and begins evaluating liveness and readiness probes. This is especially useful for JVM applications, large Spring Boot services, or apps performing migrations at startup.

### Liveness Probe
The liveness probe determines whether a container is still running correctly after it has started. If the liveness probe fails repeatedly, Kubernetes restarts the container, assuming it is stuck or in a broken state (for example, a deadlock or infinite loop). Liveness probes protect against applications that appear alive at the process level but are no longer functioning properly.

### Readiness Probe
The readiness probe controls whether a container is ready to receive traffic. If this probe fails, Kubernetes removes the Pod from Service endpoints, but does not restart the container. This allows the application to temporarily stop receiving requests during startup, configuration reloads, or dependency outages, and then resume traffic once it becomes ready again.

### How They Work Together
A typical production setup uses all three probes: the startup probe gives the application time to initialize, the readiness probe ensures traffic is only sent to healthy and prepared Pods, and the liveness probe guarantees self-healing by restarting unhealthy containers. Together, they enable zero-downtime deployments, resilient services, and stable runtime behavior in Kubernetes.

## Update Color API - Implement v1.2.0: Adding Health Endpoints

### Building and Pushing the v1.2.1 Image

```
docker build -t selcouthonism/color-api-image:1.2.1 .
docker push selcouthonism/color-api-image:1.2.1
```
The Docker build command creates a new version (1.2.1) of the color-api image that includes health endpoints such as /up, /live, and /ready, which are required for Kubernetes probes. Tagging the image with a new version ensures immutability and allows Kubernetes to detect and roll out changes safely. Pushing the image to Docker Hub makes it accessible to the Kubernetes cluster so Pods can pull and run this updated version.

#### Deploying the Pod with Probes
```
kubectl apply -f color-api-pod-probes.yaml
```
Applying color-api-pod-probes.yaml creates a Pod that defines startup and liveness probes pointing to the new health endpoints. This configuration tells Kubernetes how to determine when the container has started, whether it should be restarted, and whether it is ready to receive traffic. At this stage, the Pod lifecycle becomes probe-driven rather than relying only on container process state.

### Testing the Startup Probe
```
env:
   - name: DELAY_STARTUP
      value: "true"
```

```
startupProbe:
   httpGet:
      path: /up
      port: 80
   failureThreshold: 2
   periodSeconds: 3
```
Enabling the **DELAY_STARTUP=true** environment variable simulates a slow-starting application. The startup probe checks the ```/up``` endpoint every three seconds and allows two failures before marking the container unhealthy. During this phase, Kubernetes does not execute liveness or readiness probes, ensuring the container is not restarted while it is still legitimately starting up.

#### Observing Startup Behavior
```
kubectl get pods --watch
```
Watching the Pod status shows it remaining in a Running but not yet Ready state until the startup probe succeeds. Recreating the Pod forces Kubernetes to re-evaluate the startup probe from scratch. Describing the Pod reveals probe events that confirm Kubernetes is patiently waiting for the application to finish initializing.

### Testing the Liveness Probe
```
env:
   - name: DELAY_STARTUP
      value: "false"
   - name: FAIL_LIVENESS
      value: "true"
```
Setting FAIL_LIVENESS=true simulates a runtime failure after startup is complete. The liveness probe begins failing, which signals Kubernetes that the container is unhealthy. As a result, Kubernetes automatically restarts the container, demonstrating how liveness probes enable self-healing when applications get stuck or enter a broken state.

```
kubectl delete -f color-api-pod-probes.yaml
kubectl apply -f color-api-pod-probes.yaml
```

#### Verifying Liveness Restarts
```
kubectl describe pod color-api-pod
kubectl get pods --watch
```
By watching Pod events and describing the Pod, you can see repeated restarts triggered by liveness probe failures. This confirms that Kubernetes is actively monitoring the container’s health and taking corrective action without human intervention. The Pod stays in service only if it passes its liveness checks.

### Testing the Readiness Probe in a Deployment
```
env:
   - name: DELAY_STARTUP
      value: "false"
   - name: FAIL_LIVENESS
      value: "false"
   - name: FAIL_READINESS
      value: "true"
```
```
kubectl apply -f dev-ns.yaml -f color-api-svc-dev.yaml -f color-api-depl-probes.yaml 
```
Creating the dev namespace and deploying the color-api with a Service introduces traffic routing into the picture. Setting FAIL_READINESS=true causes some Pods to fail their readiness probes while remaining alive. These Pods continue running but are marked as NotReady, meaning they should not receive traffic.

#### Observing Readiness Effects
```
kubectl describe deployment color-api-deployment -n dev
```

```
kubectl get pods -n dev
kubectl get pods -n dev -o wide
```

Output:
```
NAME                                    READY   STATUS    RESTARTS   AGE
color-api-deployment-69b88469ff-444xg   0/1     Running   0          4m17s
color-api-deployment-69b88469ff-6fk9b   1/1     Running   0          4m17s
color-api-deployment-69b88469ff-bhfm2   0/1     Running   0          4m17s
color-api-deployment-69b88469ff-c778v   0/1     Running   0          4m17s
color-api-deployment-69b88469ff-nmxr8   1/1     Running   0          4m17s
color-api-deployment-69b88469ff-vktts   0/1     Pending   0          4m17s
```
Listing Pods shows some replicas with 0/1 READY even though their status is Running. Describing an affected Pod confirms repeated readiness probe failures with HTTP 503 responses. This demonstrates that readiness probes control traffic flow without restarting containers.

```
kubectl describe pod color-api-deployment-69b88469ff-444xg -n dev
```

Output:
```
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Warning  Unhealthy  53s (x35 over 5m58s)  kubelet            spec.containers{color-api-container}: Readiness probe failed: HTTP probe failed with statuscode: 503
```

### Deploying the Traffic Generator
```
kubectl apply -f traffic-generator.yaml 
```
The traffic generator Pod, running in the default namespace, continuously sends requests to the color-api Service. This simulates real client traffic and allows you to observe how Kubernetes Services route requests only to healthy Pods. It validates probe behavior in a realistic, multi-Pod environment.

#### Observing Traffic and Service Endpoints
```
kubectl logs traffic-generator
kubectl logs -f traffic-generator
```
Traffic generator logs show responses coming only from Pods that pass readiness checks. Describing the Service reveals that only healthy Pod IPs appear in the Endpoints list. This confirms that Kubernetes Services automatically exclude unhealthy Pods based on readiness probe results.

```
kubectl describe svc color-api-svc -n dev
```

Output:
```
Endpoints: 10.244.0.154:80,10.244.0.158:80
```

### Cleaning Up Resources
```
kubectl delete -f traffic-generator.yaml -f color-api-pod-probes.yaml -f color-api-svc-dev.yaml -f color-api-depl-probes.yaml -f dev-ns.yaml
```
Deleting all related resources removes Pods, Deployments, Services, and namespaces created during the exercise. This ensures a clean cluster state and prevents leftover resources from consuming CPU, memory, or IP addresses. Cleanup is a best practice after probe and deployment experiments.